
Top Music Sites:

https://www.billboard.com/
https://pitchfork.com/
https://www.rollingstone.com/
https://www.nme.com/

1.
etcdctl get /crawler/semaphores/cnn.com/ --prefix --keys-only

if count < 5 proceed

2.
/crawler/semaphores/cnn.com/thread-<UUID>

lease_id=$(etcdctl lease grant 300)
etcdctl put /crawler/semaphores/cnn.com/thread-<UUID> "" --lease=$lease_id

3.
Crawln the url follow anchortext but limit to the 5 domains

ex retrieval:

import requests
from bs4 import BeautifulSoup
import os

def scrape_and_save_webpage(url, output_dir):
    """Scrapes a webpage, extracts text, and saves it to a text file."""
    try:
        response = requests.get(url)
        response.raise_for_status()  # Raise an exception for bad status codes

        soup = BeautifulSoup(response.content, 'html.parser')

        # Extract text (you might need to refine this based on the specific website)
        text_content = soup.get_text(separator='\n', strip=True)

        # Create the output directory if it doesn't exist
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        # Save the text to a file
        filename = url.replace("http://", "").replace("https://", "").replace("/", "_")
        file_path = os.path.join(output_dir, filename + ".txt")
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(text_content)
        print(f"Saved {url} to {file_path}")

    except requests.exceptions.RequestException as e:
        print(f"Error fetching {url}: {e}")
    except Exception as e:
        print(f"Error processing {url}: {e}")

# Example usage:
if __name__ == "__main__":
    target_url = "https://www.example.com"
    output_directory = "web_page_data"
    scrape_and_save_webpage(target_url, output_directory)


Extract embeddings.
Store local embeddings in a database like Chroma, Pinecone, Weaviate, Qdrant etc....


# Example:
import chromadb
from sentence_transformers import SentenceTransformer

# Initialize embedding model
embedder = SentenceTransformer('all-MiniLM-L6-v2')

# Example documents
documents = [
    "Ollama lets you run LLMs locally.",
    "Vector databases are used for similarity search.",
    "Chroma is a lightweight vector DB for local use."
]

# Embed documents
embeddings = embedder.encode(documents).tolist()

# Setup Chroma DB or use hosted version https://docs.trychroma.com/docs/overview/introduction
chroma_client = chromadb.Client()
collection = chroma_client.create_collection("knowledge")

# Add documents to collection
for i, (doc, vec) in enumerate(zip(documents, embeddings)):
    collection.add(
        documents=[doc],
        embeddings=[vec],
        ids=[str(i)]
    )



Query using vector search

query = "How does Ollama work?"
query_embedding = embedder.encode([query]).tolist()

# Find top relevant context
results = collection.query(
    query_embeddings=query_embedding,
    n_results=2
)

relevant_context = "\n".join(results['documents'][0])


Call ollama with retrieved context:

import subprocess

prompt = f"""
Use the context to answer the question.

Context:
{relevant_context}

Question: {query}
"""

# Call Ollama model (e.g., mistral)
result = subprocess.run(
    ["ollama", "run", "mistral"],
    input=prompt.encode(),
    capture_output=True
)

print(result.stdout.decode())


