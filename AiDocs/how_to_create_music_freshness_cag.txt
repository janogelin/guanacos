
<top five sites>
https://www.billboard.com/
https://pitchfork.com/
https://www.rollingstone.com/
https://www.nme.com/
</top five sites>

<etcd>
use etcd to limit number of threads crawling each domain (default 5 threads)
<tasks>

store the base domains from top five sites with 5 tokens

1.
example using base domain storing site in etcd:
etcdctl get /crawler/semaphores/billboard.com/ --prefix --keys-only

limiting to 5 

2.
if count < 5 proceed

/crawler/semaphores/cnn.com/thread-<UUID>

lease_id=$(etcdctl lease grant 300)
etcdctl put /crawler/semaphores/cnn.com/thread-<UUID> "" --lease=$lease_id

3.
Crawln the url using the python program and store the output in json format




Extract embeddings.
Store local embeddings in a database locally using chroma vector DB

# Example:
import chromadb
from sentence_transformers import SentenceTransformer

# Initialize embedding model
embedder = SentenceTransformer('all-MiniLM-L6-v2')

# Example documents
documents = data from crawled json file

embeddings = Embed documents from json to embeddings

add documents to collection


4. run test program with example query
Query using vector search

query = "Latest news"
query_embedding = embedder.encode([query]).tolist()

# Find top relevant context
results = collection.query(
    query_embeddings=query_embedding,
    n_results=2
)

relevant_context = "\n".join(results['documents'][0])


Call ollama with retrieved context:

import subprocess

prompt = f"""
Use the context to answer the question.

Context:
{relevant_context}

Question: {query}
"""

# Call Ollama model (e.g., gemma3:4b)
result = subprocess.run(
    ["ollama", "run", "gemma3"],
    input=prompt.encode(),
    capture_output=True
)

print(result.stdout.decode())


